{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc4QbwpTMy3l",
        "outputId": "b01c9bde-0cf7-4371-e70b-41eea9481d56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.10.5)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install requests beautifulsoup4 pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# The base URL of the website we want to scrape\n",
        "base_url = 'http://books.toscrape.com/catalogue/'\n",
        "current_page_url = base_url + 'page-1.html'\n",
        "\n",
        "# Lists to store all the data we collect from every page\n",
        "all_books_data = []\n",
        "\n",
        "# Keep scraping as long as there is a \"next\" page\n",
        "while True:\n",
        "    print(f\"Scraping page: {current_page_url}\")\n",
        "\n",
        "    # Make the request to the current page\n",
        "    response = requests.get(current_page_url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    # Find all book \"articles\" on the current page\n",
        "    books = soup.find_all('article', class_='product_pod')\n",
        "\n",
        "    # For each book, extract the link to its detail page\n",
        "    for book in books:\n",
        "        # The link is relative, so we need to join it with the base URL\n",
        "        book_url = base_url + book.find('a')['href']\n",
        "\n",
        "        # Now, go to the book's specific page to get more details\n",
        "        book_response = requests.get(book_url)\n",
        "        book_soup = BeautifulSoup(book_response.text, 'html.parser')\n",
        "\n",
        "        # --- Extracting the detailed data ---\n",
        "        try:\n",
        "            title = book_soup.find('h1').text\n",
        "            price = book_soup.find('p', class_='price_color').text\n",
        "            stock = book_soup.find('p', class_='instock availability').text.strip()\n",
        "            rating_map = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}\n",
        "            rating_text = book_soup.find('p', class_='star-rating')['class'][1]\n",
        "            rating = rating_map.get(rating_text, 0) # Default to 0 if not found\n",
        "            category = book_soup.find('ul', class_='breadcrumb').find_all('a')[2].text\n",
        "            description = book_soup.find('div', id='product_description').find_next_sibling('p').text\n",
        "\n",
        "            # Append the dictionary of book data to our master list\n",
        "            all_books_data.append({\n",
        "                'Title': title,\n",
        "                'Price': price,\n",
        "                'Stock Availability': stock,\n",
        "                'Rating': rating,\n",
        "                'Category': category,\n",
        "                'Description': description\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"  Could not process book. Error: {e}\")\n",
        "\n",
        "    # Check if there is a \"next\" button on the page\n",
        "    next_button = soup.find('li', class_='next')\n",
        "    if next_button:\n",
        "        # If there is, get the link and prepare to scrape the next page\n",
        "        next_page_href = next_button.find('a')['href']\n",
        "        current_page_url = base_url + next_page_href\n",
        "        time.sleep(1) # Be polite and wait 1 second between page loads\n",
        "    else:\n",
        "        # If there's no \"next\" button, we're on the last page. Break the loop.\n",
        "        print(\"No more pages found. Scraping complete.\")\n",
        "        break\n",
        "\n",
        "# Create the final DataFrame from our list of dictionaries\n",
        "books_df = pd.DataFrame(all_books_data)\n",
        "\n",
        "# Save the rich dataset to a CSV file\n",
        "books_df.to_csv('books_dataset.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"\\n--- Scraping Finished! ---\")\n",
        "print(f\"Total books scraped: {len(books_df)}\")\n",
        "print(\"\\n--- Dataset Preview ---\")\n",
        "print(books_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_RvVTb2zTa-l",
        "outputId": "28e2ad7d-eda7-4c71-e4b9-268ca061a15f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page: http://books.toscrape.com/catalogue/page-1.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-2.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-3.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-4.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-5.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-6.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-7.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-8.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-9.html\n",
            "  Could not process book. Error: 'NoneType' object has no attribute 'find_next_sibling'\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-10.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-11.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-12.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-13.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-14.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-15.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-16.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-17.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-18.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-19.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-20.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-21.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-22.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-23.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-24.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-25.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-26.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-27.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-28.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-29.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-30.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-31.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-32.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-33.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-34.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-35.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-36.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-37.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-38.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-39.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-40.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-41.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-42.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-43.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-44.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-45.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-46.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-47.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-48.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-49.html\n",
            "Scraping page: http://books.toscrape.com/catalogue/page-50.html\n",
            "  Could not process book. Error: 'NoneType' object has no attribute 'find_next_sibling'\n",
            "No more pages found. Scraping complete.\n",
            "\n",
            "--- Scraping Finished! ---\n",
            "Total books scraped: 998\n",
            "\n",
            "--- Dataset Preview ---\n",
            "                                   Title    Price       Stock Availability  \\\n",
            "0                   A Light in the Attic  Â£51.77  In stock (22 available)   \n",
            "1                     Tipping the Velvet  Â£53.74  In stock (20 available)   \n",
            "2                             Soumission  Â£50.10  In stock (20 available)   \n",
            "3                          Sharp Objects  Â£47.82  In stock (20 available)   \n",
            "4  Sapiens: A Brief History of Humankind  Â£54.23  In stock (20 available)   \n",
            "\n",
            "   Rating            Category  \\\n",
            "0       3              Poetry   \n",
            "1       1  Historical Fiction   \n",
            "2       1             Fiction   \n",
            "3       4             Mystery   \n",
            "4       5             History   \n",
            "\n",
            "                                         Description  \n",
            "0  It's hard to imagine a world without A Light i...  \n",
            "1  \"Erotic and absorbing...Written with starling ...  \n",
            "2  Dans une France assez proche de la nÃ´tre, un ...  \n",
            "3  WICKED above her hipbone, GIRL across her hear...  \n",
            "4  From a renowned historian comes a groundbreaki...  \n"
          ]
        }
      ]
    }
  ]
}